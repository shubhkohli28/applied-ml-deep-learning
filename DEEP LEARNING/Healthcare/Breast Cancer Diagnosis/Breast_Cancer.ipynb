{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fb3579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8b3dd",
   "metadata": {},
   "source": [
    "We are dropping columns - 'id' and 'Unnamed: 32' as they have no role in prediction\n",
    "\n",
    "data['diagnosis'].map(): Converts the diagnosis column, which contains 'M' (Malignant) and 'B' (Benign) into binary values. 'M' is converted to 1 and 'B' to 0 making it suitable for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97b0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Dataset\n",
    "data.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Input and Output data\n",
    "y = data['diagnosis'].values\n",
    "x_data = data.drop(['diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4735d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (30, 483)\n",
      "x test:  (30, 86)\n",
      "y train:  (483,)\n",
      "y test:  (86,)\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "x = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n",
    "\n",
    "# Splitting data for training and testing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# x_train= x_train.T: Transpose(T) to ensure that data has correct shape for matrix operations during the logistic regression.\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"x train: \", x_train.shape)\n",
    "print(\"x test: \", x_test.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print(\"y test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29376d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Model Architecture\n",
    "\n",
    "\n",
    "#Initializing Weight and bias\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.random.randn(dimension, 1) * 0.01  \n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "# Sigmoid Function to calculate z value.\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward-Backward Propagation\n",
    "def forward_backward_propagation(w, b, x_train, y_train):\n",
    "    m = x_train.shape[1]\n",
    "    z = np.dot(w.T, x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    \n",
    "   \n",
    "    cost = (-1/m) * np.sum(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head))\n",
    "    \n",
    "    derivative_weight = (1/m) * np.dot(x_train, (y_head - y_train).T)\n",
    "    derivative_bias = (1/m) * np.sum(y_head - y_train)\n",
    "    \n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    return cost, gradients\n",
    "\n",
    "# Updating Parameters\n",
    "\n",
    "# Weight(w) and Bias(b) are updated by subtracting the gradient scaled by the learning rate.\n",
    "def update(w, b, x_train, y_train, learning_rate, num_iterations):\n",
    "    costs = []\n",
    "    gradients = {}\n",
    "    for i in range(num_iterations):\n",
    "        cost, grad = forward_backward_propagation(w, b, x_train, y_train)\n",
    "        w -= learning_rate * grad[\"derivative_weight\"]\n",
    "        b -= learning_rate * grad[\"derivative_bias\"]\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    parameters = {\"weight\": w, \"bias\": b}\n",
    "    return parameters, gradients, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318846e",
   "metadata": {},
   "source": [
    "Forward-Backward Propagation\n",
    "\n",
    "np.dot(w.T, x_train): Computes the matrix multiplication of the weights and the input data.\n",
    "\n",
    "cost = (-1/m) * np.sum(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head)): Measures the difference between the predicted probability (y_head) and true label (y_train).\n",
    "\n",
    "derivative_weight = (1/m) * np.dot(x_train, (y_head - y_train).T): This calculates the gradient of the cost with respect to the weights w. It tells us how much we need to change the weights to reduce the cost.\n",
    "\n",
    "derivative_bias = (1/m) * np.sum(y_head - y_train): This computes the gradient of the cost with respect to the bias b. It is simply the average of the difference between predicted probabilities (y_head) and actual labels (y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a0085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "def predict(w, b, x_test):\n",
    "    m = x_test.shape[1]\n",
    "    y_prediction = np.zeros((1, m))\n",
    "    z = sigmoid(np.dot(w.T, x_test) + b)\n",
    "\n",
    "    for i in range(z.shape[1]):\n",
    "        y_prediction[0, i] = 1 if z[0, i] > 0.5 else 0\n",
    "\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd1f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6940653448672531\n",
      "Cost after iteration 100: 0.6664684710926668\n",
      "Cost after iteration 200: 0.6419426094106131\n",
      "Cost after iteration 300: 0.6195184317848816\n",
      "Cost after iteration 400: 0.5988649785711659\n",
      "Cost after iteration 500: 0.5797903540476342\n",
      "Cost after iteration 600: 0.5621407317886556\n",
      "Cost after iteration 700: 0.5457801712896718\n",
      "Cost after iteration 800: 0.5305863581910254\n",
      "Cost after iteration 900: 0.5164492398966638\n",
      "Train accuracy: 90.47619047619048%\n",
      "Test accuracy: 88.37209302325581%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000):\n",
    "    dimension = x_train.shape[0]\n",
    "    w, b = initialize_weights_and_bias(dimension)\n",
    "    parameters, gradients, costs = update(w, b, x_train, y_train, learning_rate, num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n",
    "    \n",
    "    print(f\"Train accuracy: {100 - np.mean(np.abs(y_prediction_train - y_train)) * 100}%\")\n",
    "    print(f\"Test accuracy: {100 - np.mean(np.abs(y_prediction_test - y_test)) * 100}%\")\n",
    "\n",
    "logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
